{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW2P2S1_Resnet34.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhJVqFNTRLfC"
      },
      "source": [
        "#These libraries help to interact with the operating system and the runtime environment respectively\n",
        "import os\n",
        "import sys\n",
        "\n",
        "#Model/Training related libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "\n",
        "#Dataloader libraries\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score, multilabel_confusion_matrix\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# import torch\n",
        "import torchvision   \n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torchvision.transforms import Compose \n",
        "from torchvision import transforms\n",
        "\n",
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIdmCXOnRYuH"
      },
      "source": [
        "#Intall Kaggle API and create kaggle directory\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!mkdir .kaggle\n",
        "#This data is used to login  into your Kaggle account\n",
        "import json\n",
        "token = {\"username\":\"samruddhi98\",\"key\":\"db26269bc2e5ae4d4f8456490e50b5a8\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXEJXn8pSAI3",
        "outputId": "f7ea6480-29e2-41aa-a003-a092baeda95b"
      },
      "source": [
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "\n",
        "!cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "!kaggle config set -n path -v /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- path is now set to: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muz3FJ35SD1k",
        "outputId": "7c0a3ceb-5f99-4ae8-e444-f7ba16e9aeff"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpbdm9rgSIiV"
      },
      "source": [
        "#Download dataset .npz files from kaggle\n",
        "!kaggle competitions download -c idl-fall21-hw2p2s1-face-classification-slack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVrpsCVu7d-T"
      },
      "source": [
        "!unzip competitions/idl-fall21-hw2p2s1-face-classification/idl-fall21-hw2p2s1-face-classification.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9p0Q57dGj58"
      },
      "source": [
        "class SimpleResidualBlock_resnet(nn.Module):\n",
        "    def __init__(self, in_channel_size, stride=1):\n",
        "        super().__init__()\n",
        "        out_channel_size = int(in_channel_size*stride)\n",
        "        self.conv1 = nn.Conv2d(in_channel_size, out_channel_size, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(out_channel_size, out_channel_size, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel_size)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel_size)\n",
        "        # self.bn3 = nn.BatchNorm2d(out_channel_size)\n",
        "        if stride == 1:\n",
        "            self.shortcut = nn.Identity()\n",
        "        else:\n",
        "            self.shortcut = nn.Conv2d(in_channel_size, out_channel_size, kernel_size=1, stride=stride, bias=False)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        shortcut = self.shortcut(x)\n",
        "        # shortcut = self.bn3(shortcut)\n",
        "        out = self.relu(out + shortcut)\n",
        "        return out\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pC43NRTCcca"
      },
      "source": [
        "# This has hard-coded hidden feature sizes.\n",
        "# You can extend this to take in a list of hidden sizes as argument if you want.\n",
        "class ClassificationNetwork(nn.Module):\n",
        "    def __init__(self, in_features, num_classes, feat_dim = 1000):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = nn.Sequential(\n",
        "            # nn.Conv2d(in_features, 32, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            # nn.BatchNorm2d(32),\n",
        "            # nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_features, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            SimpleResidualBlock_resnet(64),\n",
        "            SimpleResidualBlock_resnet(64),\n",
        "            SimpleResidualBlock_resnet(64),\n",
        "            SimpleResidualBlock_resnet(64,2),\n",
        "            SimpleResidualBlock_resnet(128),\n",
        "            SimpleResidualBlock_resnet(128),\n",
        "            SimpleResidualBlock_resnet(128),\n",
        "            SimpleResidualBlock_resnet(128,2),\n",
        "            SimpleResidualBlock_resnet(256),\n",
        "            SimpleResidualBlock_resnet(256),\n",
        "            SimpleResidualBlock_resnet(256),\n",
        "            SimpleResidualBlock_resnet(256),\n",
        "            SimpleResidualBlock_resnet(256),\n",
        "            SimpleResidualBlock_resnet(256,2),\n",
        "            SimpleResidualBlock_resnet(512),\n",
        "            SimpleResidualBlock_resnet(512),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), # For each channel, collapses (averages) the entire feature map (height & width) to 1x1\n",
        "            nn.Flatten(), # the above ends up with batch_size x 64 x 1 x 1, flatten to batch_size x 64\n",
        "        )\n",
        "\n",
        "       \n",
        "        self.linear = nn.Linear(512, feat_dim)\n",
        "        # nn.init.kaiming_normal(self.linear.weight)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.linear_output = nn.Linear(512,num_classes)  \n",
        "        # nn.init.kaiming_normal(self.linear_output.weight)   \n",
        "\n",
        "    def forward(self, x, return_embedding=False):\n",
        "        embedding = self.layers(x) \n",
        "        embedding_out = self.relu(self.linear(embedding))\n",
        "        output = self.linear_output(embedding)\n",
        "        if return_embedding:\n",
        "            return embedding_out,output\n",
        "        else:\n",
        "            return output   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hILV5g486Bke"
      },
      "source": [
        "# !nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cxyQm8KCfnn",
        "outputId": "93c5f24c-0731-489c-87c4-3b497696055a"
      },
      "source": [
        "# Dataloaders\n",
        "\n",
        "train_transform = Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.RandomHorizontalFlip(p=0.4),\n",
        "    # transforms.RandomVerticalFlip(p=0.25),\n",
        "    # transforms.RandomAffine(10),\n",
        "    transforms.RandomRotation(20)\n",
        "    # transforms.RandomCrop(32, padding=4),\n",
        "    \n",
        "])\n",
        "\n",
        "test_transform = Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "train_dataset = torchvision.datasets.ImageFolder(root='train_data/', \n",
        "                                                 transform=train_transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, \n",
        "                                               shuffle=True, num_workers=8)\n",
        "\n",
        "dev_dataset = torchvision.datasets.ImageFolder(root='val_data/', \n",
        "                                               transform=test_transform)\n",
        "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=128, \n",
        "                                             shuffle=False, num_workers=8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPvTlZhPCi0u"
      },
      "source": [
        "# clearing cache to get rid of CUDA out of memory error\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "try:\n",
        "    del network\n",
        "except:\n",
        "    pass\n",
        "gc.collect()\n",
        "\n",
        "# Initializing objects\n",
        "numEpochs = 100\n",
        "in_features = 3 # RGB channels\n",
        "learningRate = 1e-1\n",
        "weightDecay = 5e-4\n",
        "num_classes = len(train_dataset.classes)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "network = ClassificationNetwork(in_features, num_classes, 1000)\n",
        "network = network.to(device)\n",
        "# summary(network, (3, 64, 64))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VB95VbS87f8",
        "outputId": "30985e95-8e88-46d0-80ca-6cf16e185ae6"
      },
      "source": [
        "# load cell (to load any saved model)\n",
        "###### SKIP THIS CELL IF YOU DO NOT WANT TO LOAD A MODEL #######\n",
        "\n",
        "epoch = 54\n",
        "val_acc = 0.9876\n",
        "PATH = 'model6_Resnet34_epoch%d_val_acc-%0.4f.pt'%(epoch,val_acc)\n",
        "print('Loading %s model '%(PATH))\n",
        "checkpoint = torch.load(PATH)\n",
        "network.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "epochs = checkpoint['epoch']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading drive/MyDrive/IDL-HW2P2-S1/model6_Resnet34_epoch54_val_acc-0.9876.pt model \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d4NOWmOClcL"
      },
      "source": [
        "# Train!\n",
        "# summary(network,(3,64,64))\n",
        "for epoch in range(34,numEpochs):\n",
        "    \n",
        "    # Train\n",
        "    network.train()\n",
        "    num_correct = 0\n",
        "    avg_loss = 0.0\n",
        "    avg_loss_epoch = []\n",
        "    for batch_num, (x, y) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = network(x)\n",
        "\n",
        "        loss = criterion(outputs, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_loss += loss.item()\n",
        "        avg_loss_epoch.append(loss.item())\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "        if batch_num % 100 == 99:\n",
        "            print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch, batch_num+1, avg_loss/100))\n",
        "            avg_loss = 0.0\n",
        "    train_acc = (num_correct / len(train_dataset))\n",
        "    print('Epoch: {} - Average training loss = {:0.4f}, Training accuracy = {:0.04f}'.format(epoch,sum(avg_loss_epoch)/len(avg_loss_epoch),train_acc))\n",
        "\n",
        "    # Save\n",
        "    PATH = 'drive/MyDrive/IDL-HW2P2-S1/model6_Resnet34_epoch%d_val_acc-%0.4f.pt'%(epoch,train_acc)\n",
        "    torch.save({\n",
        "        'epoch': numEpochs,\n",
        "        'model_state_dict': network.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict(),\n",
        "        }, PATH)\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    del x\n",
        "    del y\n",
        "    del avg_loss\n",
        "    gc.collect()\n",
        "\n",
        "    # Validate\n",
        "    network.eval()\n",
        "    num_correct = 0\n",
        "    avg_val_loss = 0\n",
        "    for batch_num, (x, y) in enumerate(dev_dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = network(x)\n",
        "        val_loss = criterion(outputs, y.long())\n",
        "        avg_val_loss += val_loss.item()\n",
        "        num_correct += (torch.argmax(outputs, axis=1) == y).sum().item()\n",
        "    val_acc = (num_correct / len(dev_dataset))\n",
        "    print('Epoch: {}, Validation Accuracy: {:4f}'.format(epoch,val_acc*100 ))\n",
        "    scheduler.step(val_acc)\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    del x\n",
        "    del y\n",
        "    del val_loss\n",
        "    gc.collect()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVtxfqCKV8z0"
      },
      "source": [
        "# Creating a class directory as ImageFolder accepts it\n",
        "!mkdir test_data/0\n",
        "!mv test_data/* test_data/0/\n",
        "\n",
        "# Test dataloader\n",
        "test_transform = Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize([0, 0, 0], [1, 1, 1])\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_dataset = torchvision.datasets.ImageFolder(root='test_data/', \n",
        "                                               transform=test_transform)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=8, \n",
        "                                             shuffle=False, num_workers=8)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvb3QAVBUBQG"
      },
      "source": [
        "# Test\n",
        "network.eval()\n",
        "num_correct = 0\n",
        "predictions = []\n",
        "for batch_num, (x, y) in enumerate(test_dataloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    outputs = network(x)\n",
        "    outputs = outputs.cpu().detach().numpy()\n",
        "    outputs = np.argmax(outputs,axis=1)\n",
        "    predictions.append(outputs)\n",
        "predictions = np.asarray(predictions).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQMT9Wxz1bjS"
      },
      "source": [
        "# Inversing the mapping and rearranging the order of predictions\n",
        "\n",
        "class_mapping = list(train_dataset.class_to_idx.items())\n",
        "predictions = list(predictions)\n",
        "preds_list = []\n",
        "for i in predictions:\n",
        "    label = [int(j[0]) for j in class_mapping if j[1] == i]\n",
        "    preds_list.append(label[0])\n",
        "\n",
        "num = list(np.arange(8000))\n",
        "num_str = list(map(lambda i:str(i),num))\n",
        "num_str.sort()\n",
        "num = list(map(lambda i:int(i),num_str))\n",
        "pred = {num[i]:preds_list[i] for i in range(len(num))}\n",
        "\n",
        "\n",
        "final_pred = []\n",
        "for i in sorted(pred.keys()):\n",
        "    final_pred.append(pred[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dml-KGu1nOV9"
      },
      "source": [
        "# Saving predictions to csv file\n",
        "import pandas as pd\n",
        "filename = 'drive/MyDrive/IDL-HW2P2-S1/submissions_res34.csv'\n",
        "index = np.arange(0,len(predictions))\n",
        "index = [str(i)+'.jpg' for i in index]\n",
        "data = {'id':(index),\n",
        "        'label':final_pred}\n",
        "pred_df = pd.DataFrame(data)\n",
        "pred_df = pred_df.rename_axis('id',axis=1)\n",
        "print(pred_df)\n",
        "pred_df.to_csv(filename,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFdqwhpScIj2",
        "outputId": "f50b16a8-9e59-4454-bbfa-4805f1095266"
      },
      "source": [
        "# Submitting predictions to kaggle\n",
        "!kaggle competitions submit -c 'idl-fall21-hw2p2s1-face-classification' -f {filename} -m \"Model-res34 submission\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 106k/106k [00:09<00:00, 12.0kB/s]\n",
            "Successfully submitted to IDL-Fall21-HW2P2S1-Face Classification"
          ]
        }
      ]
    }
  ]
}